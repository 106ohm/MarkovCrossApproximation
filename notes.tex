\documentclass[]{article}

%opening
\title{Chebyshev Approximation based Solution\\of Markov Dependability Models}
\author{}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{cleveref}
\usepackage{url}
\usepackage{color}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Problem statement}
We want to find the solution bundle $\pi(t,\theta)$ of the system of PDEs
\begin{equation}
\label{eq:PDEs}
\begin{cases}
\frac{\partial\pi(t,\theta)}{\partial t} &= Q(\theta)\pi(t,\theta),\\
\pi(0,\theta) &= \pi_0,
\end{cases}
\end{equation}
where $\pi_0\in\mathbb R^N$, $\theta\in\mathbb R^p$, 
$Q(\theta)\in\mathbb R^{N\times N}$, $t\in[0,t_f]$, and 
$\theta_i\in [l_i,u_i]$.
This system originates from the Kolmogorov Forward Equation (a system of ODEs)
$d\pi_{\theta}(t)/dt = Q_{\theta}\pi_{\theta}(t)$,
where the only independent variable is $t$, and $\theta$ is a vector of constants 
jet to be defined, i.e., parameters, that can be used to label the solutions.
If $\theta$ is upgraded to the independent variable status, i.e., when we 
are looking for the solution bundle $\pi(t,p)$, 
we obtain the system of PDEs defined in \Cref{eq:PDEs}.

\section{Observations}
First trial: exploit Chebfun\footnote{\url{https://www.chebfun.org}}.
In particular \texttt{chebop} and \texttt{chebop2}.
With \texttt{chebop} we can address the Kolmogorov Forward Equation, where $\theta$ 
are constants, because the problem is stated as the approximation of the 
solution of a system of linear ODEs.
We can address $\pi_{\theta}$ as a vector, the problem is that we cannot 
address $\theta$ as a vector of independent variables.

With \texttt{chebop2} we can address a single additional independent variable, 
i.e., $\theta\in\mathbb R$, and this is quite restrictive by itself,
but the real issue is that we cannot address $\pi$ as a vector.

\section{Approximation ``by hand''}
First consider the Kolmogorov Forward equations (system pf ODEs), i.e.,
$\theta$ is a constant, and actually we can avoid mentioning it.
Calling $T_j$ the $j$-th Chebyshev polynomials, we can define
\begin{equation}
\label{eq:hatpi1d}
\hat{\pi}_i(t) = \sum_{j=1}^n X_{ij}T_j(\varphi(t)),
\end{equation}
where $\varphi$ sends $[0,t_f]$ to $[-1,1]$, and $X\in\mathbb R^{N,n}$ is the
unknown matrix.
Observing that the $a_h$ in $T_j'=\sum_{h=0}^{j-1}a_hT_h$ are well-known constants,
defining $A_{jh}=\mathbbm{1}_{\{h<j\}}a_h$ so that $A$ is triangular, we can write
\[
\forall i. \sum_{j,h}X_{ij}A_{jh}\varphi'(t)T_h(\varphi(t))
-\sum_{l,j}Q_{il}X_{lj}T_j(\varphi(t))\approx 0\in\mathbb R,
\] 
i.e., $\forall i.X_iADT-Q_iXT\approx 0$, calling $X_i$ the $i$-th row of $X$,
$T\mathbb R^{n}$ the vector whose entries are $T_j(\varphi(t_h))$,
$t_h$ the node points, and $D$ is the diagonal matrix whose entries are
equal to $\varphi'(t_h)$. 
Then $(XDA-QX)T\approx 0\in\mathbb R^N$, that resemble a Sylvester equation.
When the initial conditions $\pi_0$ are included within the picture,
we can hope to solve this equation. 

Notice that\footnote{\url{https://arxiv.org/pdf/1409.2789.pdf}} 
in \texttt{chebop}, instead of writing $T'_j$ in terms of Chebyshev polynomial, 
the ultraspherical polynomials are employed, so that leading to a sparse matrix
instead of a triangolar one, but for now we want to simplify things as much as possible.

In \texttt{chebop2} a discretization of a separable representation of the
differential operator is considered. Here instead we have to deal just with a
1D partial derivative, and in addition the right hand side of the PDE has
a simple shape. Thus, we want to do computation ``by hand''.
Define the approximation to the solution bundle as
\begin{equation}
\label{eq:hatpi}
\hat{\pi}_i(t) = \sum_{j,k_1,\dots,k_p=1}^n 
X_{ijk_1\dots k_p}T_j(\varphi(t))
T_{k_1}(\psi_i(\theta_1))\cdots T_{k_p}(\psi_p(\theta_p)),
\end{equation}
where $\psi_i$ sends $[l_i,u_i]$ to $[-1,1]$. We have
\begin{align*}
\forall i. &\sum_{j,h,,k_1,\dots,k_p}X_{ij}A_{jh}\varphi'(t)T_h(\varphi(t))
T_{k_1}(\psi_i(\theta_1))\cdots T_{k_p}(\psi_p(\theta_p))+\\
-&\sum_{l,j,,k_1,\dots,k_p}Q_{il}X_{lj}T_j(\varphi(t))T_{k_1}(\psi_i(\theta_1))\cdots T_{k_p}(\psi_p(\theta_p))
\approx 0\in\mathbb R,
\end{align*}
but this time things are not-so-nice as before.


\section{Adaptive cross approximation}

We consider the problem of approximating the smooth functions 
\[
    m(t, \theta_1, \ldots, \theta_p) = 
      \langle 
        \pi(t, \theta_1, \ldots, \theta_p), 
        r
       \rangle,
\]
where $r \in \mathbb R^n$ is a non-negative vector, 
and $\pi(t, \theta_1, \ldots, \theta_p)$ is
a row vector defined by 
the Kolmogorov differential equation
\[
    \begin{cases}
        \dot{\pi}(t, \theta_1, \ldots, \theta_p) = \pi(t, \theta_1, \ldots, \theta_p) Q(\theta_1, \ldots, \theta_p) \\ 
        \pi(0, \theta_1, \ldots, \theta_p) = \pi_0. 
    \end{cases}
\]
The function 
$m(t, \theta)$ is a parameter--dependent instantaneous measure associated with
the continuous time
Markov chain with infinitesimal generator 
$Q(\theta_1, \ldots, \theta_p)$. Our aim is 
retrieving a uniformly accurate representation of 
$m(t, \theta_1, \ldots, \theta_p)$ without explicitly 
evaluating $\pi(\cdot)$ at all points 
of a lattice discretizing the time and the parameter space. 
Indeed, given discretization of the time $t$ and parameters 
$\theta_1, \ldots, \theta_p$ with $n_0, \ldots, n_p$ 
nodes, evaluating all the possible 
values of $\pi$ would lead to an asymptotic cost 
$\mathcal O(n_1 \ldots n_{p+1})$. 

Of course, we are also interested in accumulated measures
\[
m(t, \theta_1, \ldots, \theta_p) = 
\langle 
b(t, \theta_1, \ldots, \theta_p), 
r
\rangle,
\]
where $b$ solves
\[
\begin{cases}
\dot{b}(t, \theta_1, \ldots, \theta_p) = b(t, \theta_1, \ldots, \theta_p) Q(\theta_1, \ldots, \theta_p) + \pi_0\\ 
\pi(0, \theta_1, \ldots, \theta_p) = 0. 
\end{cases}
\]
Actually, $b_i(t, \theta_1, \ldots, \theta_p)=\int_0^t \pi_i(t, \theta_1, \ldots, \theta_p) dt$.

\subsection{The case $p = 1$}

It is instructive to start form the easiest case, where only 
one parameter $\theta_1$ is used in the model. Suppose to 
consider discretizations of $t$ and $\theta$ 
as follows:
\[
    t^{(1)} < \ldots < t^{(n_0)}, \qquad 
    \theta_1^{(1)} < \ldots < \theta_1^{(n_1)}. 
\]
The matrix defined by $X_{ij} = m(t^{(i)}, \theta_1^{(j)})$ is of 
size $n_0 \times n_1$. If the discretizations above are 
appropriately chosen, we expect to be able to recover the 
value of $m(t, \theta_1)$ at any point by interpolating it 
over the nodes (for instance using spline interpolation --- we shall 
discuss the most appropriate method for choosing the points for 
$t$ and $\theta_1$ later). 

We now make the following observation: if 
$Q(\theta_1)$ is smooth over $[\theta_1^{(\min)}, \theta_1^{(\max)}]$
then the same holds for $e^{tQ(\theta_1)}$ for any 
$\theta_1$. Therefore, we can express the measure 
$m(t, \theta_1)$ as 
\[
    m(t, \theta_1) = \langle r, \pi(t) \rangle = 
      \langle r, \pi_0 e^{tQ(\theta_1)} \rangle. 
\]
{ \color{red} Low-rank justification here }

We now consider an approach first proposed by Bebendorf 
in \cite{bebendorf2003adaptive}, and that has recently been employed in 
the successful \texttt{chebfun2} package. The idea is 
to approximate a bivariate function $m(t, \theta)$ with a 
sum of separable approximations
\[
    m(t, \theta) \approx 
      g_1(t) h_1(\theta) + g_2(t) h_2(\theta) + \ldots 
      + g_k(t) h_k(\theta). 
\]
If evaluated at the chosen nodes $t^{(i)}$ and 
$\theta_1^{(j)}$, this approximation yields a matrix 
of rank at most $k$, which can be stored efficiently, and 
whose entries (i.e., the evaluations of $m(t,\theta)$ at the 
points in the 2D lattice) are computable in $\mathcal O(k)$ 
operations. 

The approximation of the matrix with the  evaluation of the  
at the nodes is equivalent to a low-rank approximation 
problem. The evaluation of the above decompositions 
can be obtained performing the following 
steps:
\begin{enumerate}
    \item Find the element of maximum modulus  $X_{\hat i, \hat j}$
      in the matrix 
      $X_{ij}$, with the evaluations 
      $m(t^{(i)}, \theta_1^{(j)})$. 
    \item Choose as $Y_1$ as the following rank $1$ matrix,
      \[
          Y_1 := \frac{1}{X_{\hat i \hat j}} 
            (X e_{\hat i}) (e_{\hat j}^T X).
      \]
      and set $X_1 = X - Y_1$. 
    \item Go back to point $1.$ replacing $X$ with $X_1$, and repeat 
      the procedure for $k$ steps. The computed $Y_1$ at the following 
      steps are labeled as $Y_j$ for $j = 2, \ldots, k$. 
\end{enumerate}

At the end of the above algorithms, one obtains a decomposition 
\[
    X = Y_1 + \ldots + Y_k + R_k, 
\]
where $R_k$ is some residual, and $Y_i$ are rank $1$ matrices. The 
residual $R_k$ can be linked with the tail of a partial LU decomposition 
with full pivoting, and in case $X$ is approximately of rank $k$, 
it can be guaranteed to be small \cite{bebendorf2003adaptive,townsend2013extension}. 

In practice, the task in point 1\. can be rather challenging: determining the
maximum modulus entry in $X$ without looking at $\mathcal O(n_0 n_1)$ 
elements is impossible in general. However, if one 
accepts to find the maximum modulus element on the current row 
or column, the method can be linked with the LU factorization 
with partial pivoting, guaranteeing similar accuracy 
in the average case, but with a linear cost in the dimension \cite{bebendorf2003adaptive}. 

We shall discuss in detail how we deal with
point 1. in Section~\ref{sec:pivot-choice}. 

\bibliographystyle{plain}
\bibliography{biblio}



\end{document}
